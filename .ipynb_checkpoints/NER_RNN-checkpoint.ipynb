{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Build vocabularies of words and tags from datasets\"\"\"\n",
    "\n",
    "import argparse\n",
    "from collections import Counter\n",
    "import json\n",
    "import os\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--min_count_word', default=1, help=\"Minimum count for words in the dataset\", type=int)\n",
    "parser.add_argument('--min_count_tag', default=1, help=\"Minimum count for tags in the dataset\", type=int)\n",
    "parser.add_argument('--data_dir', default='data/small', help=\"Directory containing the dataset\")\n",
    "\n",
    "# Hyper parameters for the vocab\n",
    "PAD_WORD = '<pad>'\n",
    "PAD_TAG = 'O'\n",
    "UNK_WORD = 'UNK'\n",
    "\n",
    "\n",
    "def save_vocab_to_txt_file(vocab, txt_path):\n",
    "    \"\"\"Writes one token per line, 0-based line id corresponds to the id of the token.\n",
    "\n",
    "    Args:\n",
    "        vocab: (iterable object) yields token\n",
    "        txt_path: (stirng) path to vocab file\n",
    "    \"\"\"\n",
    "    with open(txt_path, \"w\") as f:\n",
    "        for token in vocab:\n",
    "            f.write(token + '\\n')\n",
    "            \n",
    "\n",
    "def save_dict_to_json(d, json_path):\n",
    "    \"\"\"Saves dict to json file\n",
    "\n",
    "    Args:\n",
    "        d: (dict)\n",
    "        json_path: (string) path to json file\n",
    "    \"\"\"\n",
    "    with open(json_path, 'w') as f:\n",
    "        d = {k: v for k, v in d.items()}\n",
    "        json.dump(d, f, indent=4)\n",
    "\n",
    "\n",
    "def update_vocab(txt_path, vocab):\n",
    "    \"\"\"Update word and tag vocabulary from dataset\n",
    "\n",
    "    Args:\n",
    "        txt_path: (string) path to file, one sentence per line\n",
    "        vocab: (dict or Counter) with update method\n",
    "\n",
    "    Returns:\n",
    "        dataset_size: (int) number of elements in the dataset\n",
    "    \"\"\"\n",
    "    with open(txt_path) as f:\n",
    "        for i, line in enumerate(f):\n",
    "            vocab.update(line.strip().split(' '))\n",
    "\n",
    "    return i + 1\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Build word vocab with train and test datasets\n",
    "    print(\"Building word vocabulary...\")\n",
    "    words = Counter()\n",
    "    size_train_sentences = update_vocab(os.path.join(args.data_dir, 'train/sentences.txt'), words)\n",
    "    size_dev_sentences = update_vocab(os.path.join(args.data_dir, 'val/sentences.txt'), words)\n",
    "    size_test_sentences = update_vocab(os.path.join(args.data_dir, 'test/sentences.txt'), words)\n",
    "    print(\"- done.\")\n",
    "\n",
    "    # Build tag vocab with train and test datasets\n",
    "    print(\"Building tag vocabulary...\")\n",
    "    tags = Counter()\n",
    "    size_train_tags = update_vocab(os.path.join(args.data_dir, 'train/labels.txt'), tags)\n",
    "    size_dev_tags = update_vocab(os.path.join(args.data_dir, 'val/labels.txt'), tags)\n",
    "    size_test_tags = update_vocab(os.path.join(args.data_dir, 'test/labels.txt'), tags)\n",
    "    print(\"- done.\")\n",
    "\n",
    "    # Assert same number of examples in datasets\n",
    "    assert size_train_sentences == size_train_tags\n",
    "    assert size_dev_sentences == size_dev_tags\n",
    "    assert size_test_sentences == size_test_tags\n",
    "\n",
    "    # Only keep most frequent tokens\n",
    "    words = [tok for tok, count in words.items() if count >= args.min_count_word]\n",
    "    tags = [tok for tok, count in tags.items() if count >= args.min_count_tag]\n",
    "\n",
    "    # Add pad tokens\n",
    "    if PAD_WORD not in words: words.append(PAD_WORD)\n",
    "    if PAD_TAG not in tags: tags.append(PAD_TAG)\n",
    "    \n",
    "    # add word for unknown words \n",
    "    words.append(UNK_WORD)\n",
    "\n",
    "    # Save vocabularies to file\n",
    "    print(\"Saving vocabularies to file...\")\n",
    "    save_vocab_to_txt_file(words, os.path.join(args.data_dir, 'words.txt'))\n",
    "    save_vocab_to_txt_file(tags, os.path.join(args.data_dir, 'tags.txt'))\n",
    "    print(\"- done.\")\n",
    "\n",
    "    # Save datasets properties in json file\n",
    "    sizes = {\n",
    "        'train_size': size_train_sentences,\n",
    "        'dev_size': size_dev_sentences,\n",
    "        'test_size': size_test_sentences,\n",
    "        'vocab_size': len(words),\n",
    "        'number_of_tags': len(tags),\n",
    "        'pad_word': PAD_WORD,\n",
    "        'pad_tag': PAD_TAG,\n",
    "        'unk_word': UNK_WORD\n",
    "    }\n",
    "    save_dict_to_json(sizes, os.path.join(args.data_dir, 'dataset_params.json'))\n",
    "\n",
    "    # Logging sizes\n",
    "    to_print = \"\\n\".join(\"- {}: {}\".format(k, v) for k, v in sizes.items())\n",
    "    print(\"Characteristics of the dataset:\\n{}\".format(to_print))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loading text data\n",
    "words_path = \"/Users/Prasann/Desktop/cs230-code-examples/pytorch/nlp/data/small/words.txt\"\n",
    "vocab = {}\n",
    "with open(words_path) as f:\n",
    "    for i, l in enumerate(f.read().splitlines()):\n",
    "        vocab[l] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tags_path = \"/Users/Prasann/Desktop/cs230-code-examples/pytorch/nlp/data/small/tags.txt\"\n",
    "tag_map = {}\n",
    "with open(tags_path) as f:\n",
    "    for i, l in enumerate(f.read().splitlines()):\n",
    "        tag_map[l] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_sentences = []        \n",
    "train_labels = []\n",
    "train_sentences_file = '/Users/Prasann/Desktop/cs230-code-examples/pytorch/nlp/data/small/train/sentences.txt'\n",
    "train_labels_file = '/Users/Prasann/Desktop/cs230-code-examples/pytorch/nlp/data/small/train/labels.txt'\n",
    "\n",
    "with open(train_sentences_file) as f:\n",
    "    for sentence in f.read().splitlines():\n",
    "        # replace each token by its index if it is in vocab\n",
    "        # else use index of UNK\n",
    "        s = [vocab[token] if token in vocab \n",
    "             else vocab['UNK']\n",
    "             for token in sentence.split(' ')]\n",
    "        train_sentences.append(s)\n",
    "    \n",
    "with open(train_labels_file) as f:\n",
    "    for sentence in f.read().splitlines():\n",
    "        # replace each label by its index\n",
    "        l = [tag_map[label] for label in sentence.split(' ')]\n",
    "        train_labels.append(l)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing a Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compute length of longest sentence in batch\n",
    "batch_max_len = max([len(s) for s in batch_sentences])\n",
    "\n",
    "# prepare a numpy array with the data, initializing the data with 'PAD' \n",
    "# and all labels with -1; initializing labels to -1 differentiates tokens \n",
    "# with tags from 'PAD' tokens\n",
    "batch_data = vocab['PAD']*np.ones((len(batch_sentences), batch_max_len))\n",
    "batch_labels = -1*np.ones((len(batch_sentences), batch_max_len))\n",
    "\n",
    "# copy the data to the numpy array\n",
    "for j in range(len(batch_sentences)):\n",
    "    cur_len = len(batch_sentences[j])\n",
    "    batch_data[j][:cur_len] = batch_sentences[j]\n",
    "    batch_labels[j][:cur_len] = batch_tags[j]\n",
    "\n",
    "# since all data are indices, we convert them to torch LongTensors\n",
    "batch_data, batch_labels = torch.LongTensor(batch_data), torch.LongTensor(batch_labels)\n",
    "\n",
    "# convert Tensors to Variables\n",
    "batch_data, batch_labels = Variable(batch_data), Variable(batch_labels)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
